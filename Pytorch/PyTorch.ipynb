{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-01T19:17:48.091919500Z",
     "start_time": "2024-04-01T19:17:35.588668200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports and GPU check\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "# Fix HTTPS certificate error\n",
    "import ssl\n",
    "ssl_create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting seed as 42"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "735dfad5b073e620"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    # Python RNG\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # PyTorch RNGs\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    # Numpy RNG\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # OS RNG\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def worker_init_fn(worker_id):    \n",
    "    \"\"\"Ensure that the data loading process is deterministic.\"\"\"\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "set_seed(42)  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T19:17:48.103581600Z",
     "start_time": "2024-04-01T19:17:48.090920100Z"
    }
   },
   "id": "6481f00b403cda36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd294368010e8d33"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "dataset_size = 5000  # Reduced dataset size for quicker training\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T19:17:48.126991500Z",
     "start_time": "2024-04-01T19:17:48.104891500Z"
    }
   },
   "id": "c3d4ce6770f0df52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the whole dataset (20gbs, takes 4 and a half minutes and freezes pc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c63f38b48fb89c31"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'import os\\nimport librosa\\n\\ndef load_data_chunked(data_dir, chunk_size=100):\\n    data = []\\n    labels = []\\n    filenames = [filename for filename in os.listdir(data_dir) if filename.endswith(\\'.wav\\')]\\n    num_files = len(filenames)\\n    num_chunks = num_files // chunk_size + 1\\n\\n    for i in range(num_chunks):\\n        start_index = i * chunk_size\\n        end_index = min((i + 1) * chunk_size, num_files)\\n        chunk_data = []\\n        chunk_labels = []\\n\\n        for filename in filenames[start_index:end_index]:\\n            file_path = os.path.join(data_dir, filename)\\n            audio, sr = librosa.load(file_path, sr=None)\\n            chunk_data.append(audio)\\n            chunk_labels.append(filename)  # Assuming labels are the filenames in this case\\n        \\n        data.append(chunk_data)\\n        labels.append(chunk_labels)\\n    \\n    return data, labels\\n\\n# Assuming your project directory structure is like this:\\n# /PyCharm_project_folder\\n#     /Data\\n#         /musicnet\\n#             /train_data\\n#             /test_data\\n#             /train_labels\\n#             /test_labels\\n\\nproject_dir = os.getcwd()\\ntrain_data_dir = os.path.join(project_dir, \\'Data\\', \\'musicnet\\',\\'musicnet\\', \\'train_data\\')\\ntest_data_dir = os.path.join(project_dir, \\'Data\\', \\'musicnet\\',\\'musicnet\\', \\'test_data\\')\\ntrain_labels_dir = os.path.join(project_dir, \\'Data\\', \\'musicnet\\',\\'musicnet\\', \\'train_labels\\')\\ntest_labels_dir = os.path.join(project_dir, \\'Data\\', \\'musicnet\\',\\'musicnet\\', \\'test_labels\\')\\n\\n# Load data in chunks\\nchunk_size = 10  # Adjust the chunk size as needed\\ntrain_data_chunks, train_labels_chunks = load_data_chunked(train_data_dir, chunk_size=chunk_size)\\ntest_data_chunks, test_labels_chunks = load_data_chunked(test_data_dir, chunk_size=chunk_size)\\n\\n# Do something with the loaded data chunks\\nfor i, (chunk_data, chunk_labels) in enumerate(zip(train_data_chunks, train_labels_chunks)):\\n    print(f\"Train data chunk {i+1}: {len(chunk_data)} files\")\\n    print(f\"Train labels chunk {i+1}: {len(chunk_labels)} labels\")\\n\\nfor i, (chunk_data, chunk_labels) in enumerate(zip(test_data_chunks, test_labels_chunks)):\\n    print(f\"Test data chunk {i+1}: {len(chunk_data)} files\")\\n    print(f\"Test labels chunk {i+1}: {len(chunk_labels)} labels\")'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "import librosa\n",
    "\n",
    "def load_data_chunked(data_dir, chunk_size=100):\n",
    "    data = []\n",
    "    labels = []\n",
    "    filenames = [filename for filename in os.listdir(data_dir) if filename.endswith('.wav')]\n",
    "    num_files = len(filenames)\n",
    "    num_chunks = num_files // chunk_size + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_index = i * chunk_size\n",
    "        end_index = min((i + 1) * chunk_size, num_files)\n",
    "        chunk_data = []\n",
    "        chunk_labels = []\n",
    "\n",
    "        for filename in filenames[start_index:end_index]:\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            audio, sr = librosa.load(file_path, sr=None)\n",
    "            chunk_data.append(audio)\n",
    "            chunk_labels.append(filename)  # Assuming labels are the filenames in this case\n",
    "        \n",
    "        data.append(chunk_data)\n",
    "        labels.append(chunk_labels)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "# Assuming your project directory structure is like this:\n",
    "# /PyCharm_project_folder\n",
    "#     /Data\n",
    "#         /musicnet\n",
    "#             /train_data\n",
    "#             /test_data\n",
    "#             /train_labels\n",
    "#             /test_labels\n",
    "\n",
    "project_dir = os.getcwd()\n",
    "train_data_dir = os.path.join(project_dir, 'Data', 'musicnet','musicnet', 'train_data')\n",
    "test_data_dir = os.path.join(project_dir, 'Data', 'musicnet','musicnet', 'test_data')\n",
    "train_labels_dir = os.path.join(project_dir, 'Data', 'musicnet','musicnet', 'train_labels')\n",
    "test_labels_dir = os.path.join(project_dir, 'Data', 'musicnet','musicnet', 'test_labels')\n",
    "\n",
    "# Load data in chunks\n",
    "chunk_size = 10  # Adjust the chunk size as needed\n",
    "train_data_chunks, train_labels_chunks = load_data_chunked(train_data_dir, chunk_size=chunk_size)\n",
    "test_data_chunks, test_labels_chunks = load_data_chunked(test_data_dir, chunk_size=chunk_size)\n",
    "\n",
    "# Do something with the loaded data chunks\n",
    "for i, (chunk_data, chunk_labels) in enumerate(zip(train_data_chunks, train_labels_chunks)):\n",
    "    print(f\"Train data chunk {i+1}: {len(chunk_data)} files\")\n",
    "    print(f\"Train labels chunk {i+1}: {len(chunk_labels)} labels\")\n",
    "\n",
    "for i, (chunk_data, chunk_labels) in enumerate(zip(test_data_chunks, test_labels_chunks)):\n",
    "    print(f\"Test data chunk {i+1}: {len(chunk_data)} files\")\n",
    "    print(f\"Test labels chunk {i+1}: {len(chunk_labels)} labels\")\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T19:17:48.127893400Z",
     "start_time": "2024-04-01T19:17:48.110811200Z"
    }
   },
   "id": "c7e352bc9b2d749f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading only 1gb of train_data "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c00b7bbcdc6bad9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: [array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([ 0.0000000e+00, -3.0517578e-05,  0.0000000e+00, ...,\n",
      "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)] [44100, 44100, 44100, 44100, 44100, 44100, 44100, 44100, 44100, 44100, 44100]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "def load_data(data_dir, max_size_bytes):\n",
    "    data = []\n",
    "    labels = []\n",
    "    total_size = 0\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.wav'):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            if total_size + file_size <= max_size_bytes:\n",
    "                audio, sr = librosa.load(file_path, sr=None)\n",
    "                data.append(audio)\n",
    "                labels.append(filename)  # Assuming labels are the filenames in this case\n",
    "                total_size += file_size\n",
    "            else:\n",
    "                break\n",
    "    return data, labels\n",
    "\n",
    "# Assuming your project directory structure is like this:\n",
    "# /PyCharm_project_folder\n",
    "#     /Data\n",
    "#         /musicnet\n",
    "#             /train_data\n",
    "#             /test_data\n",
    "#             /train_labels\n",
    "#             /test_labels\n",
    "\n",
    "project_dir = os.getcwd()\n",
    "train_data_dir = os.path.join(project_dir, 'Data', 'musicnet', 'musicnet', 'train_data')\n",
    "test_data_dir = os.path.join(project_dir, 'Data', 'musicnet', 'musicnet', 'test_data')\n",
    "train_labels_dir = os.path.join(project_dir, 'Data', 'musicnet', 'musicnet', 'train_labels')\n",
    "test_labels_dir = os.path.join(project_dir, 'Data', 'musicnet', 'musicnet', 'test_labels')\n",
    "\n",
    "# Load approximately 1GB of training data\n",
    "max_train_data_size = 1 * 1024 * 1024 * 1024  # 1GB in bytes\n",
    "train_data, train_labels = load_data(train_data_dir, max_train_data_size)\n",
    "\n",
    "# Do something with the loaded data\n",
    "print(\"Train data:\", train_data)\n",
    "print(\"Train labels:\", train_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T19:17:49.902277500Z",
     "start_time": "2024-04-01T19:17:48.121340500Z"
    }
   },
   "id": "19e2aabeb7edbfe3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: [array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([ 0.0000000e+00, -3.0517578e-05,  0.0000000e+00, ...,\n",
      "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)] [44100, 44100, 44100, 44100, 44100, 44100, 44100, 44100, 44100, 44100, 44100]\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T19:23:33.166687500Z",
     "start_time": "2024-04-01T19:23:32.454999100Z"
    }
   },
   "id": "2ad341647f2fdc8d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T19:23:33.174406Z",
     "start_time": "2024-04-01T19:23:33.162278200Z"
    }
   },
   "id": "8806a0aeef77df9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T19:23:33.520796400Z",
     "start_time": "2024-04-01T19:23:33.492322100Z"
    }
   },
   "id": "be8a975360d15482"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [40771584] at entry 0 and [30710016] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m----> 3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Forward pass\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_learning_music_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_learning_music_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_learning_music_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_learning_music_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    217\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_learning_music_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    141\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msamples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtransposed\u001B[49m\u001B[43m]\u001B[49m  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_learning_music_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    141\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_learning_music_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 121\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_learning_music_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:183\u001B[0m, in \u001B[0;36mcollate_numpy_array_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np_str_obj_array_pattern\u001B[38;5;241m.\u001B[39msearch(elem\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mstr) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    181\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format\u001B[38;5;241m.\u001B[39mformat(elem\u001B[38;5;241m.\u001B[39mdtype))\n\u001B[1;32m--> 183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_learning_music_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 121\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_learning_music_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    172\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    173\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [40771584] at entry 0 and [30710016] at entry 1"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T19:23:34.164520300Z"
    }
   },
   "id": "c084bf0cd8fbdb2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T19:23:38.879307500Z"
    }
   },
   "id": "b2e969debe345c44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
